#!/usr/bin/env bash
set -e
set -o pipefail  # Ensure pipeline failures are caught (critical for nixos-rebuild | nom)

# Cleanup handler for interrupts (Ctrl+C) and errors
TEMP_FILES=()
# shellcheck disable=SC2329  # invoked indirectly via trap
cleanup() {
    local exit_code=$?
    # Remove any temp files we created
    for f in "${TEMP_FILES[@]}"; do
        rm -f "$f" 2>/dev/null
    done
    # Remove common temp file patterns
    rm -f /tmp/nixos-build-$$ /tmp/nixbuild-output-$$.tmp /tmp/changelog-draft-$$.md /tmp/changelog-updated-$$.md /tmp/gc-output-$$.tmp 2>/dev/null
    if [ "$exit_code" -ne 0 ] && [ "$exit_code" -ne 130 ]; then
        echo ""
        echo -e "\033[0;31mâš ï¸  Rebuild interrupted or failed (exit code: $exit_code)\033[0m"
    fi
    exit "$exit_code"
}
trap cleanup EXIT INT TERM

# Mark that we're running inside the rebuild-nixos wrapper
# This allows Claude Code hooks to permit nixos-rebuild calls from within this script
export NIXOS_REBUILD_WRAPPER=1

# Source user secrets from environment.d if not already in environment
# This handles the case where the script is run before re-login after adding secrets
if [ -z "$ANTHROPIC_API_KEY" ] && [ -f ~/.config/environment.d/50-secrets.conf ]; then
    # shellcheck source=/dev/null
    source ~/.config/environment.d/50-secrets.conf
    # Export so subprocesses (like nix run) can see it
    export ANTHROPIC_API_KEY
fi

# Parse command line arguments
VERBOSE=false
DRY_RUN=false
QUICK=false
YES=false
AUDIT=false
VERIFY_BOOTSTRAP=false
FRESH=false

while [[ $# -gt 0 ]]; do
    case "$1" in
        --verbose|-v) VERBOSE=true; shift ;;
        --dry-run|-n) DRY_RUN=true; shift ;;
        --quick|-q) QUICK=true; shift ;;
        --yes|-y) YES=true; shift ;;
        --audit|-a) AUDIT=true; shift ;;
        --verify-bootstrap) VERIFY_BOOTSTRAP=true; shift ;;
        --fresh|-f) FRESH=true; shift ;;
        --help|-h)
            echo "Usage: rebuild-nixos [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  -v, --verbose    Show detailed output during build"
            echo "  -n, --dry-run    Show what would happen without making changes"
            echo "  -q, --quick      Skip security checks and cleanup (fastest rebuild)"
            echo "  -y, --yes        Auto-accept all prompts (non-interactive)"
            echo "  -f, --fresh      Clear and bypass eval-cache (use when changes seem ignored)"
            echo "  -a, --audit      Export source closure for forensic audit trail (~5GB)"
            echo "  --verify-bootstrap  Deep reproducibility check of bootstrap packages"
            echo "  -h, --help       Show this help message"
            echo ""
            echo "Security:"
            echo "  By default, rebuild-nixos runs supply chain security checks:"
            echo "    - Git vs tarball comparison (detects XZ-style backdoors)"
            echo "    - Reproducibility verification of critical packages"
            echo "  Use --quick to skip these checks, --audit for full forensic export."
            echo ""
            echo "Examples:"
            echo "  ./rebuild-nixos              # Full rebuild with security checks"
            echo "  ./rebuild-nixos --quick      # Fastest: skip security + cleanup"
            echo "  ./rebuild-nixos --yes        # Auto-accept all prompts"
            echo "  ./rebuild-nixos --fresh      # Bypass eval-cache for clean rebuild"
            echo "  ./rebuild-nixos --audit      # Include forensic audit trail export"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Setup logging
LOG_DIR="$HOME/.claude/.logs"
mkdir -p "$LOG_DIR"
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
LOG_FILE="$LOG_DIR/rebuild-$TIMESTAMP.log"

# Colors for output
BLUE='\033[0;34m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Tracking arrays for summary
declare -a WARNINGS=()
declare -a ACTIONS=()
declare -a STATS=()

# Helper functions
log_step() {
    local step_time
    step_time=$(date +%H:%M:%S)
    echo -e "${BLUE}[$step_time]${NC} $1"
}

log_success() {
    echo -e "${GREEN}âœ…${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}âš ï¸${NC}  $1"
    WARNINGS+=("$1")
}

log_error() {
    echo -e "${RED}âŒ${NC} $1"
}

add_action() {
    ACTIONS+=("$1")
}

add_stat() {
    STATS+=("$1")
}

# Major step indicator with "Step X/Y" format
# Usage: step "Step description"
step() {
    CURRENT_STEP=$((CURRENT_STEP + 1))
    local step_name="$1"
    echo ""
    echo -e "${BLUE}â”â”â” Step $CURRENT_STEP/$TOTAL_STEPS: $step_name â”â”â”${NC}"
}

# Prompt user with auto-accept support
# Usage: prompt_user "Question?" "default" â†’ returns 0 (yes) or 1 (no)
# With --yes flag: auto-accepts (returns 0)
prompt_user() {
    local prompt="$1"
    if [ "$YES" = true ]; then
        echo -e "$prompt ${GREEN}[auto-accepted]${NC}"
        return 0  # Yes
    fi

    read -p "$prompt " -n 1 -r
    echo
    [[ $REPLY =~ ^[Yy]$ ]]
}

# Dry-run helper - shows what would run without executing
dry_run_skip() {
    local description="$1"
    if [ "$DRY_RUN" = true ]; then
        echo -e "${YELLOW}[DRY-RUN]${NC} Would: $description"
        return 0
    fi
    return 1
}

# Progress indicator with time estimates and status
show_progress() {
    local step_name="$1"
    local pid="$2"
    local history_file="$LOG_DIR/build-times.log"
    local start_time
    start_time=$(date +%s)

    # Calculate average from last 5 builds if available
    local avg_time=""
    if [ -f "$history_file" ]; then
        avg_time=$(tail -5 "$history_file" | awk '{sum+=$1; count++} END {if(count>0) print int(sum/count)}')
    fi

    local spinstr='â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â '
    local last_status=""

    while kill -0 "$pid" 2>/dev/null; do
        local elapsed=$(($(date +%s) - start_time))
        local mins=$((elapsed / 60))
        local secs=$((elapsed % 60))

        # Build progress message
        local msg
        msg=$(printf "â±ï¸  %02d:%02d" "$mins" "$secs")

        # Add ETA if we have historical data
        if [ -n "$avg_time" ] && [ "$avg_time" -gt 0 ]; then
            local remaining=$((avg_time - elapsed))
            if [ $remaining -gt 0 ]; then
                local eta_mins=$((remaining / 60))
                local eta_secs=$((remaining % 60))
                msg=$(printf "%s | ~%02d:%02d remaining" "$msg" "$eta_mins" "$eta_secs")
            fi
        fi

        # Try to get current status from log file
        if [ -f "$LOG_FILE" ]; then
            local current_status
            current_status=$(tail -5 "$LOG_FILE" 2>/dev/null | grep -E "building|copying|installing|activating|running|switching|updating" | tail -1)

            if [ -n "$current_status" ]; then
                # Extract package name from nix store path if present
                if echo "$current_status" | grep -q "/nix/store/"; then
                    # Extract just the package name (hash-packagename.drv -> packagename)
                    local pkg_name
                    pkg_name=$(echo "$current_status" | grep -oP '/nix/store/[a-z0-9]+-\K[^/]+' | head -1 | sed 's/\.drv$//')
                    if [ -n "$pkg_name" ]; then
                        # Get the action (building, copying, etc.)
                        local action
                        action=$(echo "$current_status" | grep -oP '^[a-z]+' | head -1)
                        last_status="$action $pkg_name"
                    else
                        last_status=$(echo "$current_status" | cut -c1-60)
                    fi
                else
                    last_status=$(echo "$current_status" | cut -c1-60)
                fi
            fi
        fi

        # Show status if available
        local status_display=""
        if [ -n "$last_status" ]; then
            status_display=" | ${last_status}"
        fi

        # Show spinner and message (allow long lines)
        local spinchar="${spinstr:0:1}"
        spinstr="${spinstr:1}${spinchar}"
        printf "\r%s  %s%s" "$spinchar" "$msg" "$status_display"
        tput el  # Clear to end of line
        sleep 0.2
    done

    # Clear progress line
    printf "\r"
    tput el

    # Record build time for future estimates
    local total_time=$(($(date +%s) - start_time))
    echo "$total_time" >> "$history_file"

    # Keep only last 10 builds
    if [ -f "$history_file" ]; then
        tail -10 "$history_file" > "$history_file.tmp"
        mv "$history_file.tmp" "$history_file"
    fi
}

# Step counter for "Step X/Y" progress indicator
# Dynamically count phases based on flags and available tools
TOTAL_STEPS=4  # Core: Update inputs, Build, Activate, Update Claude configs
# nvd diff preview adds a step (nvd is installed on this system)
command -v nvd &>/dev/null && TOTAL_STEPS=$((TOTAL_STEPS + 1))
# Adaptive learning adds a step unless --quick
[ "$QUICK" != true ] && TOTAL_STEPS=$((TOTAL_STEPS + 1))
# Bootstrap verification adds a step with --verify-bootstrap
[ "$VERIFY_BOOTSTRAP" = true ] && TOTAL_STEPS=$((TOTAL_STEPS + 1))
CURRENT_STEP=0

# Track total rebuild duration
REBUILD_START_TIME=$(date +%s)

# Detect hostname for flake configuration (avoids hardcoding "nixos")
NIXOS_HOSTNAME=$(hostname)
if ! nix eval ".#nixosConfigurations.\"$NIXOS_HOSTNAME\"" --apply 'x: true' &>/dev/null; then
    # No matching flake config for this hostname â€” list available and fail
    AVAILABLE_HOSTS=$(nix flake show --json 2>/dev/null | jq -r '.nixosConfigurations | keys[]' 2>/dev/null || true)
    log_error "No flake configuration found for hostname '$NIXOS_HOSTNAME'"
    if [ -n "$AVAILABLE_HOSTS" ]; then
        echo ""
        echo "Available configurations:"
        echo "$AVAILABLE_HOSTS" | while read -r host; do echo "  - $host"; done
        echo ""
        echo "Re-run with the correct hostname set, or add a config for '$NIXOS_HOSTNAME' to flake.nix"
    fi
    exit 1
fi

# Start rebuild
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘            NixOS System Rebuild & Configuration            â•‘"
echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
if [ "$DRY_RUN" = true ]; then
echo "â•‘  Mode: Dry-run (no changes will be made)                   â•‘"
elif [ "$QUICK" = true ] && [ "$YES" = true ]; then
echo "â•‘  Mode: Quick + Auto-accept (fastest)                       â•‘"
elif [ "$QUICK" = true ]; then
echo "â•‘  Mode: Quick (skip cleanup phases)                         â•‘"
elif [ "$YES" = true ]; then
echo "â•‘  Mode: Auto-accept (non-interactive)                       â•‘"
else
echo "â•‘  Mode: Full (interactive)                                  â•‘"
fi
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

if [ "$VERBOSE" = true ]; then
    echo "ðŸ” Verbose mode enabled"
    echo "ðŸ“ Logs: $LOG_FILE"
fi

if [ "$DRY_RUN" = true ]; then
    echo "ðŸ” Dry-run mode: showing what would happen without making changes"
    echo ""
fi

# Cache sudo credentials early (activation step needs sudo)
if [ "$DRY_RUN" = false ]; then
    echo "ðŸ” This rebuild requires sudo privileges for system activation."
    sudo -v || { log_error "Failed to obtain sudo access"; exit 1; }
fi

# === PHASE 0: Eval-Cache Management ===
# Manages Nix eval-cache to prevent stale cache from causing "changes ignored" issues
# The eval-cache stores evaluated Nix expressions and can become stale over time,
# causing rebuilds to incorrectly report "no changes" even when flake.nix was modified.
# This is the root cause of the "phantom generation" problem.
#
# IMPORTANT: Also manages ~/.local/share/nix/trusted-settings.json which can contain
# a "eval-cache": {"true": true} entry that OVERRIDES --option eval-cache false!
# Exit on failure: No (cache clearing is best-effort)

# Dynamic eval-cache directory detection (handles v5, v6, future versions)
# Finds the latest versioned eval-cache directory
EVAL_CACHE_DIR=$(find "$HOME/.cache/nix" -maxdepth 1 -type d -name "eval-cache-v*" 2>/dev/null | sort -V | tail -1)
if [ -z "$EVAL_CACHE_DIR" ]; then
    EVAL_CACHE_DIR="$HOME/.cache/nix/eval-cache-v6"  # Fallback to current version
fi
TRUSTED_SETTINGS="$HOME/.local/share/nix/trusted-settings.json"
CACHE_MAX_AGE_DAYS=7

manage_eval_cache() {
    local cache_cleared=false
    local reason=""

    # Case 1: --fresh flag - always clear cache
    if [ "$FRESH" = true ]; then
        reason="--fresh flag requested"
        cache_cleared=true
    # Case 2: Auto-detect stale cache (files older than threshold)
    elif [ -d "$EVAL_CACHE_DIR" ]; then
        # Find files older than threshold
        local stale_files
        stale_files=$(find "$EVAL_CACHE_DIR" -type f -mtime +"$CACHE_MAX_AGE_DAYS" 2>/dev/null | head -1)
        if [ -n "$stale_files" ]; then
            # Get the oldest file's age for reporting
            local oldest_file
            oldest_file=$(find "$EVAL_CACHE_DIR" -type f -printf '%T+ %p\n' 2>/dev/null | sort | head -1 | cut -d' ' -f2)
            if [ -n "$oldest_file" ]; then
                local oldest_age
                oldest_age=$(stat -c %Y "$oldest_file" 2>/dev/null)
                local now
                now=$(date +%s)
                local age_days=$(( (now - oldest_age) / 86400 ))
                reason="stale cache detected (oldest entry: ${age_days} days old)"
                cache_cleared=true
            fi
        fi
    fi

    # Clear eval-cache directory (handles all versioned directories when --fresh)
    if [ "$cache_cleared" = true ]; then
        # Clear ALL eval-cache versions when --fresh is used
        for cache_dir in "$HOME"/.cache/nix/eval-cache-v*; do
            if [ -d "$cache_dir" ]; then
                local cache_size
                cache_size=$(du -sh "$cache_dir" 2>/dev/null | cut -f1)
                if [ "$DRY_RUN" = true ]; then
                    echo -e "${YELLOW}[DRY-RUN]${NC} Would clear $(basename "$cache_dir") (${cache_size}): ${reason}"
                else
                    rm -rf "$cache_dir"
                    log_step "Cleared $(basename "$cache_dir") (${cache_size}): ${reason}"
                    add_action "Cleared stale eval-cache to ensure fresh evaluation"
                fi
            fi
        done
    elif [ "$VERBOSE" = true ] && [ -d "$EVAL_CACHE_DIR" ]; then
        local cache_size
        cache_size=$(du -sh "$EVAL_CACHE_DIR" 2>/dev/null | cut -f1)
        log_step "Eval-cache (${cache_size}) is fresh (< ${CACHE_MAX_AGE_DAYS} days old)"
    fi

    # CRITICAL: ALWAYS remove eval-cache from trusted-settings.json if present
    # This setting can OVERRIDE --option eval-cache false, causing phantom generation issues
    # Runs on EVERY rebuild, not just --fresh, to prevent the setting from accumulating
    if [ -f "$TRUSTED_SETTINGS" ]; then
        if grep -q '"eval-cache"' "$TRUSTED_SETTINGS" 2>/dev/null; then
            if [ "$DRY_RUN" = true ]; then
                echo -e "${YELLOW}[DRY-RUN]${NC} Would remove eval-cache from trusted-settings.json"
            else
                # Use jq if available, otherwise sed
                if command -v jq &>/dev/null; then
                    local tmp
                    tmp=$(mktemp)
                    jq 'del(."eval-cache")' "$TRUSTED_SETTINGS" > "$tmp" && mv "$tmp" "$TRUSTED_SETTINGS"
                else
                    # Fallback: remove eval-cache entry with sed
                    sed -i 's/,"eval-cache":{[^}]*}//g; s/"eval-cache":{[^}]*},//g' "$TRUSTED_SETTINGS"
                fi
                log_step "Removed eval-cache from trusted-settings.json (prevents --fresh override)"
                add_action "Removed eval-cache trusted setting to prevent cache override"
            fi
        fi
    fi
}

# Run eval-cache management
manage_eval_cache

# === PHASE 1: Update Flake Inputs ===
# Updates nix flake lock file to get latest package versions
# SMART UPDATE: Uses --refresh for locally-maintained repos to bypass cache
# Side effects: Modifies flake.lock, may pull in breaking changes
# Exit on failure: Yes (can't build without valid inputs)

step "Updating flake inputs"

if dry_run_skip "update flake inputs (nixpkgs, home-manager, local repos)"; then
    : # skip
else
    # Define locally-maintained inputs that need cache bypass
    LOCAL_INPUTS=(
        "code-cursor-nix"
        "whisper-dictation"
        "claude-automation"
        "claude-code-nix"  # Also locally maintained by you
        "antigravity-nix"
    )

    if [ "$VERBOSE" = true ]; then
        echo "ðŸ“¦ Updating external inputs (using cache)..."
        nix flake update nixpkgs home-manager 2>&1 | tee -a "$LOG_FILE"

        echo "ðŸš€ Updating local inputs (bypassing cache with --refresh)..."
        # Batch update: single nix invocation for all local inputs (faster than per-input)
        nix flake update "${LOCAL_INPUTS[@]}" --refresh 2>&1 | tee -a "$LOG_FILE"
    else
        # Quiet mode: Update external inputs first (cached)
        nix flake update nixpkgs home-manager &>>"$LOG_FILE"

        # Batch update local inputs with --refresh to bypass cache
        nix flake update "${LOCAL_INPUTS[@]}" --refresh &>>"$LOG_FILE"
    fi

    log_success "Flake inputs updated (local repos refreshed)"
    add_action "Updated flake.lock with cache bypass for local repos"
fi

# === PHASE 2: Test Build ===
# Builds new configuration WITHOUT activating it (safe to fail)
# This catches build errors before touching running system
# Exit on failure: Yes (don't activate broken config)
# Perform a test build
step "Building configuration"

# Check if nom (nix-output-monitor) is available for beautiful build visualization
if command -v nom &>/dev/null; then
    USE_NOM=true
else
    USE_NOM=false
    [ "$VERBOSE" = false ] && log_warning "nom not found - using basic progress indicator (install nix-output-monitor for better UX)"
fi

if [ "$VERBOSE" = true ]; then
    # Verbose mode - show all output
    NEW_CONFIG_PATH=$(NIXPKGS_ALLOW_UNFREE=1 nix build --no-link --print-out-paths --impure .#nixosConfigurations."$NIXOS_HOSTNAME".config.system.build.toplevel 2>&1 | tee -a "$LOG_FILE" | tail -1)
elif [ "$USE_NOM" = true ]; then
    # Use nix-output-monitor with explicit JSON mode for clean tree-only output
    # --log-format internal-json: nix outputs JSON (not raw text)
    # nom --json: parses JSON and shows ONLY the dependency tree
    # NOTE: -v flag removed - it was causing verbose derivation list to be dumped
    # The internal-json format works without -v and gives nom what it needs
    BUILD_LINK="/tmp/nixos-build-$$"
    echo ""
    NIXPKGS_ALLOW_UNFREE=1 nix build --log-format internal-json --out-link "$BUILD_LINK" --impure .#nixosConfigurations."$NIXOS_HOSTNAME".config.system.build.toplevel 2>&1 | nom --json 2>&1 | tee -a "$LOG_FILE"
    BUILD_EXIT=${PIPESTATUS[0]}

    if [ "$BUILD_EXIT" -ne 0 ]; then
        log_error "Test build failed (exit code: $BUILD_EXIT)"
        echo "Check logs: $LOG_FILE"
        rm -f "$BUILD_LINK"
        exit 1
    fi

    # Get path from symlink and clean up
    NEW_CONFIG_PATH=$(readlink -f "$BUILD_LINK")
    rm -f "$BUILD_LINK"
else
    # Fallback: custom progress indicator (when nom not available)
    NIXPKGS_ALLOW_UNFREE=1 nix build --no-link --print-out-paths --impure .#nixosConfigurations."$NIXOS_HOSTNAME".config.system.build.toplevel > /tmp/nixbuild-output-$$.tmp 2>>"$LOG_FILE" &
    BUILD_PID=$!
    show_progress "test-build" $BUILD_PID
    wait $BUILD_PID
    BUILD_EXIT=$?

    if [ $BUILD_EXIT -ne 0 ]; then
        log_error "Test build failed (exit code: $BUILD_EXIT)"
        echo "Check logs: $LOG_FILE"
        rm -f /tmp/nixbuild-output-$$.tmp
        exit 1
    fi

    NEW_CONFIG_PATH=$(cat /tmp/nixbuild-output-$$.tmp)
    rm -f /tmp/nixbuild-output-$$.tmp
fi

log_success "Test build successful"

# === PHASE 2.5: Package Diff Preview ===
# Shows what packages will change before activation (using nvd)
# This helps users understand the impact before committing
if command -v nvd &>/dev/null; then
    CURRENT_SYSTEM=$(readlink -f /run/current-system)
    step "Analyzing package changes"
    echo ""

    # Run nvd diff and capture output
    NVD_OUTPUT=$(nvd diff "$CURRENT_SYSTEM" "$NEW_CONFIG_PATH" 2>/dev/null || true)

    if [ -n "$NVD_OUTPUT" ]; then
        # Count changes - nvd uses format: [A+] added, [R-] removed, [U â†‘] upgrade, [D â†“] downgrade
        # Example: "[A+]  #1  linggen   <none>"
        # NOTE: grep -c outputs "0" AND exits 1 when no matches, so || echo "0" would give "00"
        # Fix: Use separate assignment to avoid concatenating both outputs
        PKG_ADDED=$(echo "$NVD_OUTPUT" | grep -c '\[A[+]\]' 2>/dev/null) || PKG_ADDED=0
        PKG_REMOVED=$(echo "$NVD_OUTPUT" | grep -c '\[R[-]\]' 2>/dev/null) || PKG_REMOVED=0
        PKG_UPGRADED=$(echo "$NVD_OUTPUT" | grep -c '\[U' 2>/dev/null) || PKG_UPGRADED=0
        PKG_DOWNGRADED=$(echo "$NVD_OUTPUT" | grep -c '\[D' 2>/dev/null) || PKG_DOWNGRADED=0

        echo -e "  ${GREEN}+${NC} Added: $PKG_ADDED  ${RED}-${NC} Removed: $PKG_REMOVED  ${BLUE}â†‘${NC} Upgraded: $PKG_UPGRADED  ${YELLOW}â†“${NC} Downgraded: $PKG_DOWNGRADED"
        echo ""

        # Show first 20 changes
        CHANGE_LINES=$(echo "$NVD_OUTPUT" | wc -l)
        CHANGE_LINES="${CHANGE_LINES//[^0-9]/}"
        if [ "$CHANGE_LINES" -gt 20 ]; then
            echo "$NVD_OUTPUT" | head -20
            echo "  ... and $((CHANGE_LINES - 20)) more changes"
        else
            echo "$NVD_OUTPUT"
        fi
        echo ""

        # Store for summary (single line, no newlines)
        PKG_CHANGES="${PKG_ADDED} added, ${PKG_REMOVED} removed, ${PKG_UPGRADED} upgraded"
        add_stat "Packages: $PKG_CHANGES"
    else
        echo "  No package changes detected"
        PKG_CHANGES=""
    fi
fi

# === PHASE 2.7: Supply Chain Security Checks ===
# Security-first approach: detect XZ-style backdoors by DEFAULT
# Based on: https://nixcademy.com/posts/secure-supply-chain-with-nix/
#
# Default mode: Runs security checks (fast, ~3-5 min)
#   - Git vs Tarball comparison (catches XZ-style attacks)
#   - Reproducibility verification
#
# --audit flag: Adds forensic audit trail (slow, 30-60 min)
#   - Source manifest export
#   - Full closure export (~5GB)
#
# --quick flag: Skips security checks (fastest rebuild)

if [ "$QUICK" = true ]; then
    log_step "Skipping supply chain security checks (--quick mode)"
else
    log_step "Running supply chain security checks..."

    AUDIT_DIR="$HOME/.nixos-audit"
    mkdir -p "$AUDIT_DIR"
    SECURITY_ISSUES=0

    # Resolve flake-pinned nixpkgs store path (not the system channel)
    # This ensures security checks target the SAME package set we're building
    # Uses nix flake archive which is fast (~60ms) and gives input store paths
    FLAKE_NIXPKGS=$(nix flake archive --json 2>/dev/null | jq -r '.inputs.nixpkgs.path' 2>/dev/null || true)
    if [ -n "$FLAKE_NIXPKGS" ] && [ -d "$FLAKE_NIXPKGS" ]; then
        log_step "Using flake-pinned nixpkgs: $(basename "$FLAKE_NIXPKGS")"
        NIXPKGS_EXPR="(import $FLAKE_NIXPKGS {})"
    else
        log_warning "Could not resolve flake-pinned nixpkgs, falling back to <nixpkgs>"
        FLAKE_NIXPKGS=""
        NIXPKGS_EXPR="(import <nixpkgs> {})"
    fi

    # === PHASE 2.7.1: Git vs Tarball Comparison (PRIMARY SECURITY CHECK) ===
    # Detects XZ-style attacks where malicious code is in tarballs but not git
    # This check would have caught CVE-2024-3094
    log_step "Checking for XZ-style backdoors (git vs tarball comparison)..."

    GIT_TARBALL_DIR="$AUDIT_DIR/git-tarball-compare-$TIMESTAMP"
    mkdir -p "$GIT_TARBALL_DIR"

    # Critical packages with their git repositories
    declare -A CRITICAL_PKG_REPOS=(
        ["xz"]="https://github.com/tukaani-project/xz"
        ["gzip"]="https://git.savannah.gnu.org/git/gzip.git"
        ["bzip2"]="https://gitlab.com/bzip2/bzip2.git"
        ["openssh"]="https://github.com/openssh/openssh-portable"
        ["openssl"]="https://github.com/openssl/openssl"
    )

    GIT_TARBALL_PASSED=0
    GIT_TARBALL_FAILED=0
    GIT_TARBALL_SKIPPED=0

    for pkg in "${!CRITICAL_PKG_REPOS[@]}"; do
        GIT_REPO="${CRITICAL_PKG_REPOS[$pkg]}"
        PKG_WORK_DIR="$GIT_TARBALL_DIR/$pkg"
        mkdir -p "$PKG_WORK_DIR"

        echo -n "  Checking $pkg... "

        # Get the package version from nixpkgs
        VERSION=$(nix eval --impure --expr "${NIXPKGS_EXPR}.${pkg}.version" 2>/dev/null | tr -d '"') || true

        if [ -z "$VERSION" ]; then
            echo "skipped (couldn't get version)"
            GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
            continue
        fi

        # Fetch and unpack the tarball source used by nixpkgs
        TARBALL_PATH=$(nix-build --expr "${NIXPKGS_EXPR}.${pkg}.src" --no-out-link 2>/dev/null) || true
        if [ -z "$TARBALL_PATH" ]; then
            echo "skipped (couldn't fetch tarball)"
            GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
            continue
        fi

        # Unpack if it's a compressed file
        TARBALL_DIR="$PKG_WORK_DIR/tarball"
        mkdir -p "$TARBALL_DIR"
        if [ -f "$TARBALL_PATH" ]; then
            case "$TARBALL_PATH" in
                *.tar.xz|*.txz)  tar -xJf "$TARBALL_PATH" -C "$TARBALL_DIR" --strip-components=1 2>/dev/null ;;
                *.tar.gz|*.tgz)  tar -xzf "$TARBALL_PATH" -C "$TARBALL_DIR" --strip-components=1 2>/dev/null ;;
                *.tar.bz2|*.tbz) tar -xjf "$TARBALL_PATH" -C "$TARBALL_DIR" --strip-components=1 2>/dev/null ;;
                *.tar)           tar -xf "$TARBALL_PATH" -C "$TARBALL_DIR" --strip-components=1 2>/dev/null ;;
                *)
                    echo "skipped (unknown archive format)"
                    GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
                    continue
                    ;;
            esac
        elif [ -d "$TARBALL_PATH" ]; then
            TARBALL_DIR="$TARBALL_PATH"
        else
            echo "skipped (source not found)"
            GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
            continue
        fi

        if [ ! -d "$TARBALL_DIR" ] || [ -z "$(ls -A "$TARBALL_DIR" 2>/dev/null)" ]; then
            echo "skipped (extraction failed)"
            GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
            continue
        fi

        # Clone git repo at the same version tag
        # Different projects use different tag formats, so we try multiple
        GIT_DIR="$PKG_WORK_DIR/git"

        # Build list of tag formats to try for this package
        case "$pkg" in
            openssh)
                # OpenSSH uses V_X_Y_P1 format (e.g., V_9_9_P1 for 9.9p1)
                TAG_NORMALIZED=$(echo "$VERSION" | tr '.' '_' | sed 's/p/_P/g' | tr '[:lower:]' '[:upper:]')
                TAG_FORMATS="V_$TAG_NORMALIZED V_$(echo "$VERSION" | tr '.' '_') v$VERSION $VERSION"
                ;;
            bzip2)
                # bzip2 uses bzip2-X.Y.Z format
                TAG_FORMATS="bzip2-$VERSION v$VERSION $VERSION"
                ;;
            openssl)
                # OpenSSL uses openssl-X.Y.Z format
                TAG_FORMATS="openssl-$VERSION v$VERSION $VERSION"
                ;;
            *)
                TAG_FORMATS="v$VERSION $VERSION"
                ;;
        esac

        # Try each tag format until one works
        for GIT_TAG in $TAG_FORMATS; do
            if timeout 60 git clone --depth 1 --branch "$GIT_TAG" "$GIT_REPO" "$GIT_DIR" 2>/dev/null; then
                break
            fi
        done

        if [ ! -d "$GIT_DIR" ]; then
            echo "skipped (couldn't clone git at version $VERSION)"
            GIT_TARBALL_SKIPPED=$((GIT_TARBALL_SKIPPED + 1))
            continue
        fi

        # Compare source files, excluding auto-generated content
        DIFF_LOG="$PKG_WORK_DIR/diff.log"

        # Auto-generated directories to exclude (gnulib, autotools, etc.)
        EXCLUDE_DIRS="lib/|build-aux/|gnulib/|gnulib-lib/|m4/|po/|doc/|tests/"
        # Auto-generated files to exclude
        EXCLUDE_FILES="ltmain.sh|config.sub|config.guess|config.h.in|aclocal.m4|Makefile.in|compile|depcomp|install-sh|missing|test-driver|ylwrap"

        # Find source files in tarball (excluding auto-generated paths)
        find "$TARBALL_DIR" -type f \( -name "*.c" -o -name "*.h" -o -name "*.sh" -o -name "*.S" \) \
            ! -name "config.h" ! -name "configure" 2>/dev/null | \
            grep -vE "$EXCLUDE_DIRS" | \
            grep -vE "$EXCLUDE_FILES" | \
            sort > "$PKG_WORK_DIR/tarball-files.txt"

        # Find source files in git
        find "$GIT_DIR" -type f \( -name "*.c" -o -name "*.h" -o -name "*.sh" -o -name "*.S" \) \
            ! -path "*/.git/*" 2>/dev/null | \
            grep -vE "$EXCLUDE_DIRS" | \
            sort > "$PKG_WORK_DIR/git-files.txt"

        SUSPICIOUS_FILES=0
        TARBALL_ONLY_FILES=""

        # Check for tarball-only files (potential hidden malicious code)
        while IFS= read -r tarball_file; do
            base_name=$(basename "$tarball_file")
            # Skip known auto-generated files
            if echo "$base_name" | grep -qE "^($EXCLUDE_FILES)$"; then
                continue
            fi
            if ! grep -q "/$base_name\$" "$PKG_WORK_DIR/git-files.txt" 2>/dev/null; then
                if file "$tarball_file" 2>/dev/null | grep -qE "text|script"; then
                    TARBALL_ONLY_FILES="$TARBALL_ONLY_FILES$base_name "
                    SUSPICIOUS_FILES=$((SUSPICIOUS_FILES + 1))
                fi
            fi
        done < "$PKG_WORK_DIR/tarball-files.txt"

        # Count content differences in matching files
        CONTENT_DIFFS=0
        TOTAL_COMPARED=0
        while IFS= read -r tarball_file; do
            base_name=$(basename "$tarball_file")
            git_file=$(grep "/$base_name\$" "$PKG_WORK_DIR/git-files.txt" 2>/dev/null | head -1)
            if [ -n "$git_file" ] && [ -f "$git_file" ]; then
                TOTAL_COMPARED=$((TOTAL_COMPARED + 1))
                if ! diff -q "$tarball_file" "$git_file" >/dev/null 2>&1; then
                    echo "DIFFERS: $base_name" >> "$DIFF_LOG"
                    CONTENT_DIFFS=$((CONTENT_DIFFS + 1))
                fi
            fi
        done < "$PKG_WORK_DIR/tarball-files.txt"

        # Calculate diff percentage (only flag if >20% of files differ)
        if [ "$TOTAL_COMPARED" -gt 0 ]; then
            DIFF_PERCENT=$((CONTENT_DIFFS * 100 / TOTAL_COMPARED))
        else
            DIFF_PERCENT=0
        fi

        # Flag as suspicious only if:
        # - Any tarball-only source files in main tree (not auto-generated), OR
        # - More than 20% of comparable files differ (suggests major tampering)
        if [ "$SUSPICIOUS_FILES" -gt 0 ]; then
            echo -e "${RED}SUSPICIOUS${NC} ($SUSPICIOUS_FILES tarball-only source files)"
            echo "    Tarball-only files: $TARBALL_ONLY_FILES" >> "$DIFF_LOG"
            log_warning "$pkg: Found $SUSPICIOUS_FILES source files in tarball not in git!"
            GIT_TARBALL_FAILED=$((GIT_TARBALL_FAILED + 1))
        elif [ "$DIFF_PERCENT" -gt 20 ]; then
            echo -e "${RED}SUSPICIOUS${NC} ($CONTENT_DIFFS/$TOTAL_COMPARED files differ - ${DIFF_PERCENT}%)"
            log_warning "$pkg: ${DIFF_PERCENT}% of source files differ from git!"
            GIT_TARBALL_FAILED=$((GIT_TARBALL_FAILED + 1))
        elif [ "$CONTENT_DIFFS" -gt 0 ]; then
            echo -e "${YELLOW}MINOR DIFFS${NC} ($CONTENT_DIFFS/$TOTAL_COMPARED files differ - ${DIFF_PERCENT}%)"
            GIT_TARBALL_PASSED=$((GIT_TARBALL_PASSED + 1))
        else
            echo -e "${GREEN}CLEAN âœ“${NC}"
            GIT_TARBALL_PASSED=$((GIT_TARBALL_PASSED + 1))
        fi

        rm -rf "$GIT_DIR"
    done

    add_stat "Git vs Tarball: $GIT_TARBALL_PASSED clean, $GIT_TARBALL_FAILED suspicious, $GIT_TARBALL_SKIPPED skipped"

    if [ "$GIT_TARBALL_FAILED" -gt 0 ]; then
        log_error "SECURITY WARNING: Some packages have suspicious tarball-only content!"
        log_warning "This is how the XZ backdoor (CVE-2024-3094) was hidden."
        log_warning "Review: $GIT_TARBALL_DIR"
        SECURITY_ISSUES=$((SECURITY_ISSUES + GIT_TARBALL_FAILED))
    fi

    # === PHASE 2.7.2: Reproducibility Check ===
    log_step "Checking critical package reproducibility..."

    CRITICAL_PKGS="xz gzip bzip2 coreutils"
    REPRO_PASSED=0
    REPRO_FAILED=0
    REPRO_SKIPPED=0

    for pkg in $CRITICAL_PKGS; do
        PKG_PATH=$(nix-build --expr "${NIXPKGS_EXPR}.${pkg}" --no-out-link 2>/dev/null) || true

        if [ -n "$PKG_PATH" ]; then
            CHECK_OUTPUT=$(nix-build --expr "${NIXPKGS_EXPR}.${pkg}" --check 2>&1) || true

            if echo "$CHECK_OUTPUT" | grep -q "checking outputs"; then
                log_success "$pkg: reproducible âœ“"
                REPRO_PASSED=$((REPRO_PASSED + 1))
            elif echo "$CHECK_OUTPUT" | grep -q "differs"; then
                log_warning "$pkg: OUTPUT DIFFERS - check manually"
                REPRO_FAILED=$((REPRO_FAILED + 1))
            else
                log_success "$pkg: verified âœ“"
                REPRO_PASSED=$((REPRO_PASSED + 1))
            fi
        else
            echo "  â­ï¸  $pkg: skipped (not in store)"
            REPRO_SKIPPED=$((REPRO_SKIPPED + 1))
        fi
    done

    add_stat "Reproducibility: $REPRO_PASSED passed, $REPRO_FAILED failed, $REPRO_SKIPPED skipped"

    if [ "$REPRO_FAILED" -gt 0 ]; then
        log_warning "Some packages failed reproducibility check - review before continuing"
        SECURITY_ISSUES=$((SECURITY_ISSUES + REPRO_FAILED))
    fi

    # === PHASE 2.7.3: Security Summary ===
    echo ""
    if [ "$SECURITY_ISSUES" -eq 0 ]; then
        log_success "All security checks passed!"
        add_action "Supply chain security checks passed"
    else
        log_error "Found $SECURITY_ISSUES security issue(s) - review before proceeding!"
        add_action "Supply chain security checks: $SECURITY_ISSUES issue(s) found"
    fi
fi
# End of default security checks (skipped with --quick)

# === PHASE 2.7.4-2.7.5: Audit Trail Export (--audit flag only) ===
# These are slow forensic operations, only run when explicitly requested
if [ "$AUDIT" = true ]; then
    # Ensure AUDIT_DIR and TIMESTAMP are set if we skipped security checks
    AUDIT_DIR="${AUDIT_DIR:-$HOME/.nixos-audit}"
    mkdir -p "$AUDIT_DIR"
    # TIMESTAMP is set once at script start â€” reuse it for consistent naming

    # === PHASE 2.7.4: Source Manifest (lightweight) ===
    log_step "Extracting source derivations for manifest..."
    FODS=$(nix derivation show -r "$NEW_CONFIG_PATH" 2>/dev/null | \
        jq -r 'to_entries[] | select(.value.outputs.out.hash != null) | .key')

    FOD_COUNT=$(echo "$FODS" | grep -c '^' || echo "0")
    echo "$FODS" > "$AUDIT_DIR/sources-$TIMESTAMP.manifest"
    log_success "Source manifest saved: $AUDIT_DIR/sources-$TIMESTAMP.manifest ($FOD_COUNT sources)"
    add_stat "Source derivations: $FOD_COUNT"

    # === PHASE 2.7.5: Optional Closure Export (audit trail) ===
    # Only offer after security checks, with appropriate warning
    echo ""
    if [ "$SECURITY_ISSUES" -gt 0 ]; then
        log_warning "Security issues detected above. Closure export is for forensic analysis only."
        EXPORT_PROMPT="Export source closure for forensic analysis? (y/n)"
    else
        EXPORT_PROMPT="Export source closure for audit trail (~5GB, 30-60 min)? (y/n)"
    fi

    if prompt_user "$EXPORT_PROMPT"; then
        log_step "Realizing source derivations (this may take a while)..."

        TIMEOUT_PER_DRV=${AUDIT_TIMEOUT_PER_DRV:-60}
        OVERALL_TIMEOUT=${AUDIT_OVERALL_TIMEOUT:-14400}

        CLOSURE_FILE="$AUDIT_DIR/source-closure-$TIMESTAMP.nar"
        REALIZE_LOG="$AUDIT_DIR/realize-$TIMESTAMP.log"
        OUTPUT_PATHS_FILE="$AUDIT_DIR/output-paths-$TIMESTAMP.txt"

        log_step "Building output-to-derivation mapping for $FOD_COUNT derivations..."
        DRV_MAP_FILE="$AUDIT_DIR/drv-map-$TIMESTAMP.txt"
        echo "$FODS" | while read -r drv; do
            out=$(nix derivation show "$drv" 2>/dev/null | jq -r '.[].outputs.out.path' 2>/dev/null)
            if [ -n "$out" ]; then
                echo "$out $drv"
            fi
        done > "$DRV_MAP_FILE"
        OUTPUT_COUNT=$(wc -l < "$DRV_MAP_FILE")
        log_success "Mapped $OUTPUT_COUNT output paths"

        cut -d' ' -f1 "$DRV_MAP_FILE" > "$OUTPUT_PATHS_FILE"

        log_step "Realizing outputs (timeout: ${TIMEOUT_PER_DRV}s per item, ${OVERALL_TIMEOUT}s total)..."

        REALIZED=0
        FAILED=0
        SKIPPED=0
        START_TIME=$(date +%s)

        while read -r output_path drv_path; do
            ELAPSED=$(($(date +%s) - START_TIME))
            if [ "$ELAPSED" -ge "$OVERALL_TIMEOUT" ]; then
                log_warning "Overall timeout reached (${OVERALL_TIMEOUT}s). Stopping realization."
                break
            fi

            if [ -e "$output_path" ]; then
                SKIPPED=$((SKIPPED + 1))
            else
                if timeout "$TIMEOUT_PER_DRV" nix-store --realise "$drv_path" >> "$REALIZE_LOG" 2>&1; then
                    REALIZED=$((REALIZED + 1))
                else
                    EXIT_CODE=$?
                    if [ "$EXIT_CODE" -eq 124 ]; then
                        echo "[TIMEOUT] $drv_path" >> "$REALIZE_LOG"
                    else
                        echo "[FAILED:$EXIT_CODE] $drv_path" >> "$REALIZE_LOG"
                    fi
                    FAILED=$((FAILED + 1))
                fi
            fi

            TOTAL=$((REALIZED + FAILED + SKIPPED))
            if [ $((TOTAL % 100)) -eq 0 ]; then
                ELAPSED=$(($(date +%s) - START_TIME))
                printf "  Progress: %d/%d (realized: %d, skipped: %d, failed: %d) [%ds elapsed]\n" \
                    "$TOTAL" "$OUTPUT_COUNT" "$REALIZED" "$SKIPPED" "$FAILED" "$ELAPSED"
            fi
        done < "$DRV_MAP_FILE"

        TOTAL_TIME=$(($(date +%s) - START_TIME))
        log_success "Realization complete: $REALIZED realized, $SKIPPED already present, $FAILED failed (${TOTAL_TIME}s)"

        if [ "$FAILED" -gt 0 ]; then
            log_warning "Some derivations failed. Check log: $REALIZE_LOG"
        fi

        log_step "Exporting realized outputs to NAR..."
        # Filter existing output paths to a temp file (avoids subshell variable scoping)
        EXPORT_LIST=$(mktemp)
        while read -r output_path; do
            if [ -e "$output_path" ]; then
                echo "$output_path"
            fi
        done < "$OUTPUT_PATHS_FILE" > "$EXPORT_LIST"
        EXPORT_COUNT=$(wc -l < "$EXPORT_LIST")

        xargs nix-store --export < "$EXPORT_LIST" > "$CLOSURE_FILE" 2>> "$REALIZE_LOG"
        rm -f "$EXPORT_LIST"

        if [ -f "$CLOSURE_FILE" ]; then
            CLOSURE_SIZE=$(du -h "$CLOSURE_FILE" | cut -f1)
            log_success "Source closure exported: $CLOSURE_FILE ($CLOSURE_SIZE, $EXPORT_COUNT paths)"
            add_action "Exported source closure for offline verification"
        else
            log_error "Failed to create closure file"
        fi

        add_stat "Realized: $REALIZED, Skipped: $SKIPPED, Failed: $FAILED"
    fi

    add_action "Audit trail exported (source manifest + closure)"
fi

# === PHASE 2.8: Bootstrap Package Verification ===
# Deep reproducibility verification of bootstrap-critical packages
# Inspired by: https://luj.fr/blog/how-nixos-could-have-detected-xz.html
if [ "$VERIFY_BOOTSTRAP" = true ]; then
    step "Deep verification of bootstrap packages"

    # Ensure NIXPKGS_EXPR is set (may be unset if --quick was also passed)
    if [ -z "${NIXPKGS_EXPR:-}" ]; then
        FLAKE_NIXPKGS=$(nix flake archive --json 2>/dev/null | jq -r '.inputs.nixpkgs.path' 2>/dev/null || true)
        if [ -n "$FLAKE_NIXPKGS" ] && [ -d "$FLAKE_NIXPKGS" ]; then
            NIXPKGS_EXPR="(import $FLAKE_NIXPKGS {})"
        else
            NIXPKGS_EXPR="(import <nixpkgs> {})"
        fi
    fi

    VERIFY_DIR="$HOME/.nixos-audit/bootstrap-verify-$TIMESTAMP"
    mkdir -p "$VERIFY_DIR"

    # Bootstrap-critical packages (first-stage build dependencies)
    BOOTSTRAP_PKGS=(
        "xz"           # Compression (xz backdoor target)
        "gzip"         # Compression
        "bzip2"        # Compression
        "coreutils"    # Basic utilities
        "gnugrep"      # Pattern matching
        "gnused"       # Stream editor
        "gawk"         # Text processing
        "bash"         # Shell
        "gnumake"      # Build system
        "binutils"     # Linker/assembler
    )

    declare -a VERIFY_RESULTS=()

    for pkg in "${BOOTSTRAP_PKGS[@]}"; do
        echo -n "  Verifying $pkg... "

        # Build with --check to compare against existing store path
        CHECK_OUTPUT=$(nix-build --expr "${NIXPKGS_EXPR}.${pkg}" --check --keep-failed 2>&1) || true

        if echo "$CHECK_OUTPUT" | grep -q "output .* is identical"; then
            echo -e "${GREEN}REPRODUCIBLE âœ“${NC}"
            VERIFY_RESULTS+=("$pkg:PASS")
        elif echo "$CHECK_OUTPUT" | grep -q "output .* differs"; then
            echo -e "${RED}DIVERGENT OUTPUT - POTENTIAL TAMPERING${NC}"
            VERIFY_RESULTS+=("$pkg:FAIL")

            # Find the .check path for analysis
            CHECK_PATH=$(echo "$CHECK_OUTPUT" | grep -oP '/nix/store/[^"]+\.check' | head -1)
            if [ -n "$CHECK_PATH" ]; then
                echo "$pkg diverged. Check path: $CHECK_PATH" >> "$VERIFY_DIR/divergent-packages.txt"

                # Run nix-diff for root cause if available
                if command -v nix-diff &>/dev/null; then
                    ORIG_PATH="${CHECK_PATH%.check}"
                    nix-diff "$ORIG_PATH" "$CHECK_PATH" > "$VERIFY_DIR/$pkg-diff.txt" 2>&1 || true
                fi
            fi
        else
            echo -e "${YELLOW}skipped (not in store or build failed)${NC}"
            VERIFY_RESULTS+=("$pkg:SKIP")
        fi
    done

    # Summary report
    PASS_COUNT=$(printf '%s\n' "${VERIFY_RESULTS[@]}" | grep -c ':PASS' || echo 0)
    FAIL_COUNT=$(printf '%s\n' "${VERIFY_RESULTS[@]}" | grep -c ':FAIL' || echo 0)
    SKIP_COUNT=$(printf '%s\n' "${VERIFY_RESULTS[@]}" | grep -c ':SKIP' || echo 0)

    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘         Bootstrap Verification Summary                     â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  Reproducible: $PASS_COUNT"
    echo "  Divergent:    $FAIL_COUNT"
    echo "  Skipped:      $SKIP_COUNT"

    if [ "$FAIL_COUNT" -gt 0 ]; then
        log_error "SECURITY ALERT: $FAIL_COUNT package(s) failed reproducibility check!"
        echo "  Review: $VERIFY_DIR/divergent-packages.txt"

        if ! prompt_user "Continue despite reproducibility failures? (y/n)"; then
            log_error "Aborting due to reproducibility failures"
            exit 1
        fi
    fi

    # Save results
    printf '%s\n' "${VERIFY_RESULTS[@]}" > "$VERIFY_DIR/results.txt"
    add_stat "Bootstrap verification: $PASS_COUNT/$((PASS_COUNT + FAIL_COUNT)) reproducible"
    add_action "Bootstrap packages verified"
fi

# === PHASE 3: Activate Configuration ===
# Switches running system to new configuration (requires sudo)
# This is the critical step - system state changes here
# Exit on failure: Yes (activation failed, system unchanged)
# Rollback available: Use 'sudo nixos-rebuild switch --rollback'

# Build extra options based on flags
NIXOS_REBUILD_OPTS=""
if [ "$FRESH" = true ]; then
    NIXOS_REBUILD_OPTS="--option eval-cache false"
    log_step "Fresh mode: eval-cache cleared (Phase 0) + bypassing cache writes"
fi

# Activate the new configuration
step "Activating configuration"

# Refresh sudo credentials before activation (may have expired during security checks)
if [ "$DRY_RUN" = false ]; then
    sudo -v || { log_error "Failed to refresh sudo access"; exit 1; }
fi

if dry_run_skip "activate new NixOS configuration (nixos-rebuild switch)"; then
    : # skip
else
    if [ "$VERBOSE" = true ]; then
        # Verbose mode - show all output, capture exit code properly
        # shellcheck disable=SC2086  # NIXOS_REBUILD_OPTS may contain multiple flags
        sudo NIXPKGS_ALLOW_UNFREE=1 nixos-rebuild switch --flake . --impure $NIXOS_REBUILD_OPTS 2>&1 | tee -a "$LOG_FILE"
        ACTIVATE_EXIT=${PIPESTATUS[0]}
    elif [ "$USE_NOM" = true ]; then
        # Use nix-output-monitor for beautiful activation visualization
        echo ""
        # shellcheck disable=SC2086  # NIXOS_REBUILD_OPTS may contain multiple flags
        sudo NIXPKGS_ALLOW_UNFREE=1 nixos-rebuild switch --flake . --impure $NIXOS_REBUILD_OPTS 2>&1 | nom | tee -a "$LOG_FILE"
        ACTIVATE_EXIT=${PIPESTATUS[0]}
    else
        # Fallback: custom progress indicator (capture both stdout and stderr)
        # shellcheck disable=SC2086,SC2024  # word splitting intended; redirect on backgrounded process
        sudo NIXPKGS_ALLOW_UNFREE=1 nixos-rebuild switch --flake . --impure $NIXOS_REBUILD_OPTS >>"$LOG_FILE" 2>&1 &
        ACTIVATE_PID=$!
        show_progress "activation" $ACTIVATE_PID
        wait $ACTIVATE_PID
        ACTIVATE_EXIT=$?
    fi

    # Check activation result
    if [ "$ACTIVATE_EXIT" -ne 0 ]; then
        echo ""  # Ensure we're on a new line
        log_error "Activation failed (exit code: $ACTIVATE_EXIT)"
        echo "ðŸ“ Check logs: $LOG_FILE"
        exit 1
    fi

    echo ""  # Ensure we're on a new line after progress indicator

    # Verify activation succeeded - check the current generation
    NEW_GEN=$(readlink /nix/var/nix/profiles/system | sed 's/system-\([0-9]*\)-link/\1/' || true)
    CURRENT_SYSTEM=$(readlink /run/current-system)

    if [ -n "$NEW_GEN" ]; then
        log_success "Configuration activated (generation $NEW_GEN)"
        log_step "System: $(basename "$CURRENT_SYSTEM")"
        add_action "NixOS configuration activated (gen $NEW_GEN)"
    else
        log_warning "Could not determine current generation"
        log_step "Current: $(basename "$CURRENT_SYSTEM")"
    fi
fi

# === PHASE 3.5: Community Reproducibility Check (r13y) ===
# Checks packages against r13y.com reproducibility database
# Only runs when --audit is enabled
if [ "$AUDIT" = true ]; then
    AUTOMATION_FLAKE="$HOME/claude-nixos-automation"
    if [ -d "$AUTOMATION_FLAKE" ]; then
        log_step "Checking against community reproducibility data..."
        if nix run "$AUTOMATION_FLAKE#check-reproducibility" 2>&1 | tee -a "$LOG_FILE"; then
            add_action "r13y reproducibility check completed"
        else
            log_warning "r13y check failed (non-critical)"
        fi
    else
        log_warning "claude-nixos-automation not found, skipping r13y check"
    fi
fi

# === PHASE 4: Update Claude Code Configurations ===
# Regenerates 5 Claude context files (CLAUDE.md, permissions, MCP, etc.)
# Uses claude-nixos-automation flake (local if available, GitHub fallback)
# Exit on failure: No (continue with warnings - not critical to system)
# Update Claude Code configurations
step "Updating Claude configs"

if dry_run_skip "update Claude Code configurations (CLAUDE.md, permissions, MCP analytics)"; then
    AUTOMATION_EXIT=1  # Skip the nested phases 5-6
else
    echo ""

    # Use local flake for automation (development mode)
    AUTOMATION_FLAKE="$HOME/claude-nixos-automation"

    if [ "$VERBOSE" = true ]; then
    # Verbose mode: show all output
    if [ -d "$AUTOMATION_FLAKE" ]; then
        if (cd ~/nixos-config && nix run "$AUTOMATION_FLAKE#update-all" 2>&1 | tee -a "$LOG_FILE"); then
            AUTOMATION_EXIT=0
        else
            AUTOMATION_EXIT=$?
        fi
    else
        # Fallback to GitHub
        if (cd ~/nixos-config && nix run github:jacopone/claude-nixos-automation#update-all 2>&1 | tee -a "$LOG_FILE"); then
            AUTOMATION_EXIT=0
        else
            AUTOMATION_EXIT=$?
        fi
    fi
else
    # Quiet mode: redirect to log, show only summary
    if [ -d "$AUTOMATION_FLAKE" ]; then
        if (cd ~/nixos-config && nix run "$AUTOMATION_FLAKE#update-all" &>"$LOG_FILE.automation"); then
            AUTOMATION_EXIT=0
        else
            AUTOMATION_EXIT=$?
        fi
    else
        # Fallback to GitHub
        if (cd ~/nixos-config && nix run github:jacopone/claude-nixos-automation#update-all &>"$LOG_FILE.automation"); then
            AUTOMATION_EXIT=0
        else
            AUTOMATION_EXIT=$?
        fi
    fi

    # Note: Tool counts removed - MCP-NixOS provides dynamic package queries
    # Keeping only file existence check for debugging
    if [ -f "./CLAUDE.md" ]; then
        add_stat "Claude configs updated"
    fi

    # Check automation log for warnings
    if [ -f "$LOG_FILE.automation" ]; then
        WARNING_COUNT=$(grep -c "WARNING:" "$LOG_FILE.automation" 2>/dev/null || true)
        WARNING_COUNT=${WARNING_COUNT:-0}

        if [ "$WARNING_COUNT" -gt 0 ]; then
            log_warning "$WARNING_COUNT warnings during automation (see $LOG_FILE.automation)"
        fi
    fi
fi

if [ $AUTOMATION_EXIT -eq 0 ]; then
    echo ""
    log_success "Claude Code configurations updated"
    add_action "Updated 5 Claude configuration files"

    # Show detailed file update summary
    echo ""
    echo "ðŸ“„ Files updated by automation:"
    echo ""

    # Function to show file status with meaningful diff summary
    show_file_status() {
        local file="$1"
        local description="$2"
        local scope="$3"

        if [ -f "$file" ]; then
            local size
            size=$(du -h "$file" 2>/dev/null | cut -f1)
            local size_bytes
            size_bytes=$(stat -c%s "$file" 2>/dev/null || stat -f%z "$file" 2>/dev/null || echo "0")
            local status="âœ“"
            local change_indicator=""

            # Sanity check: 0 bytes = broken
            if [ -z "$size_bytes" ] || [ "$size_bytes" -eq 0 ]; then
                echo -e "  ${RED}âœ— $description (BROKEN - 0 bytes!)${NC}"
                echo "     â†³ $scope"
                return
            fi

            # Check if file is in git and what changed
            if git ls-files --error-unmatch "$file" &>/dev/null 2>&1; then
                if git diff --quiet "$file" 2>/dev/null; then
                    change_indicator=" ${GREEN}unchanged${NC}"
                    echo -e "  $status $description [$size]$change_indicator"
                    echo "     â†³ $scope"
                else
                    # Count lines changed
                    local changes
                    changes=$(git diff --numstat "$file" 2>/dev/null | awk '{print "+"$1" -"$2}')
                    if [ -n "$changes" ]; then
                        change_indicator=" ${YELLOW}updated${NC} ($changes lines)"
                    else
                        change_indicator=" ${YELLOW}updated${NC}"
                    fi
                    echo -e "  $status $description [$size]$change_indicator"
                    echo "     â†³ $scope"

                    # Show meaningful diff summary (what actually changed)
                    # Extract key changes: section headers, important lines
                    # Get added section headers (## lines that were added)
                    local added_sections
                    added_sections=$(git diff "$file" 2>/dev/null | grep "^+##" | sed 's/^+/  + /' | head -3)
                    # Get removed section headers
                    local removed_sections
                    removed_sections=$(git diff "$file" 2>/dev/null | grep "^-##" | sed 's/^-/  - /' | head -3)
                    # Get other significant additions (non-empty, non-header lines)
                    local added_content
                    added_content=$(git diff "$file" 2>/dev/null | grep "^+[^+]" | grep -v "^+##" | grep -v "^+$" | grep -v "^+---" | sed 's/^+//' | head -2)
                    # Get significant removals
                    local removed_content
                    removed_content=$(git diff "$file" 2>/dev/null | grep "^-[^-]" | grep -v "^-##" | grep -v "^-$" | grep -v "^----" | sed 's/^-//' | head -2)

                    # Display changes
                    if [ -n "$added_sections" ]; then
                        echo -e "     ${GREEN}Added sections:${NC}"
                        echo "$added_sections" | while read -r line; do
                            [ -n "$line" ] && echo -e "       ${GREEN}$line${NC}"
                        done
                    fi
                    if [ -n "$removed_sections" ]; then
                        echo -e "     ${RED}Removed sections:${NC}"
                        echo "$removed_sections" | while read -r line; do
                            [ -n "$line" ] && echo -e "       ${RED}$line${NC}"
                        done
                    fi
                    if [ -n "$added_content" ] && [ -z "$added_sections" ]; then
                        echo -e "     ${GREEN}Key additions:${NC}"
                        echo "$added_content" | head -2 | while read -r line; do
                            [ -n "$line" ] && echo -e "       ${GREEN}+ ${line:0:60}${NC}"
                        done
                    fi
                    if [ -n "$removed_content" ] && [ -z "$removed_sections" ]; then
                        echo -e "     ${RED}Key removals:${NC}"
                        echo "$removed_content" | head -2 | while read -r line; do
                            [ -n "$line" ] && echo -e "       ${RED}- ${line:0:60}${NC}"
                        done
                    fi
                fi
            else
                change_indicator=" ${BLUE}untracked${NC}"
                echo -e "  $status $description [$size]$change_indicator"
                echo "     â†³ $scope"
            fi
        else
            echo -e "  ${RED}âœ— $description (FILE NOT FOUND - automation failed!)${NC}"
            echo "     â†³ $scope"
        fi
    }

    # System-wide files (global, all projects)
    show_file_status "$HOME/.claude/CLAUDE.md" \
        "User policies & conventions" \
        "Global: Tool selection rules, policies (MCP-NixOS for package queries)"

    show_file_status "$HOME/.claude/CLAUDE-USER-POLICIES.md" \
        "Your personal policies" \
        "Global: Git commit rules, rebuild restrictions, doc standards"

    echo ""

    # Project-specific files (this project only)
    show_file_status "./CLAUDE.md" \
        "Project context" \
        "Local: Tech stack, commands, architecture (trackable in git)"

    show_file_status "./.claude/settings.local.json" \
        "Security permissions" \
        "Local: Auto-generated allow/deny rules for this project"

    show_file_status "./.claude/CLAUDE.local.md" \
        "Machine state" \
        "Local: Hardware info, services, git branches (gitignored)"

    echo ""

    # === PHASE 5: MCP Usage Analytics ===
    # Analyzes MCP server utilization across all sessions
    # Marks sessions as ANALYZED (Phase 2 lifecycle)
    # Generates .claude/mcp-analytics.md report
    # Exit on failure: No (non-critical analytics)
    # Run MCP usage analytics (marks sessions as ANALYZED - Phase 2)
    log_step "Analyzing MCP servers and marking sessions..."
    if [ -d "$AUTOMATION_FLAKE" ]; then
        MCP_CMD="nix run $AUTOMATION_FLAKE#update-mcp-usage-analytics"
    else
        MCP_CMD="nix run github:jacopone/claude-nixos-automation#update-mcp-usage-analytics"
    fi

    if (cd ~/nixos-config && $MCP_CMD 2>/dev/null); then
        log_success "MCP analysis completed"
        echo "  â†³ Sessions automatically marked as ANALYZED (Phase 2 lifecycle)"
    else
        log_warning "MCP analysis had issues (non-critical)"
    fi

    # Extract MCP stats (even if command failed, use existing file)
    if [ -f ~/nixos-config/.claude/mcp-analytics.md ]; then
        # Extract stats from mcp-analytics.md (match actual template format)
        # Note: Use || true to prevent pipefail from exiting on no matches
        TOTAL_CONFIGURED=$(grep "^### Configured Servers" ~/nixos-config/.claude/mcp-analytics.md 2>/dev/null | grep -oP '\(\K\d+' || true)
        TOTAL_CONFIGURED=${TOTAL_CONFIGURED:-0}
        MCP_CONNECTED=$(grep "^- âœ… \*\*Connected\*\*:" ~/nixos-config/.claude/mcp-analytics.md 2>/dev/null | grep -oP '\d+' | head -1 || true)
        MCP_CONNECTED=${MCP_CONNECTED:-0}

        # Add to statistics (simplified - no session count since it's not tracked)
        add_stat "MCP: $MCP_CONNECTED/$TOTAL_CONFIGURED connected"
    fi

    # === PHASE 6: Tool Usage Analytics ===
    # Tracks which system tools are actually used vs merely installed
    # Enables data-driven decisions on tool adoption
    # Exit on failure: No (non-critical analytics)
    # Run tool usage analytics (Phase 1)
    log_step "Analyzing system tool usage..."
    if [ -d "$AUTOMATION_FLAKE" ]; then
        TOOL_CMD="nix run $AUTOMATION_FLAKE#update-tool-analytics --"
    else
        TOOL_CMD="nix run github:jacopone/claude-nixos-automation#update-tool-analytics --"
    fi

    if (cd ~/nixos-config && $TOOL_CMD 2>/dev/null); then
        # Show brief tool usage status from tool-analytics.md
        TOOL_ANALYTICS="./.claude/tool-analytics.md"
        if [ -f "$TOOL_ANALYTICS" ]; then
            # Extract stats from tool-analytics.md Summary section
            # Format: "- **Total Tools Installed**: 129" and "- **Tools Used**: 35 (27.1%)"
            TOOL_TOTAL=$(grep "Total Tools Installed" "$TOOL_ANALYTICS" | grep -oP '\d+' | head -1 || true)
            TOOL_TOTAL=${TOOL_TOTAL:-"?"}
            TOOL_USED=$(grep "Tools Used" "$TOOL_ANALYTICS" | grep -oP '\d+' | head -1 || true)
            TOOL_USED=${TOOL_USED:-"?"}
            TOOL_ADOPTION=$(grep "Tools Used" "$TOOL_ANALYTICS" | grep -oP '\((\d+)' | grep -oP '\d+' || true)
            TOOL_ADOPTION=${TOOL_ADOPTION:-"?"}

            # Display with clearer format
            if [ "$TOOL_USED" != "?" ] && [ "$TOOL_TOTAL" != "?" ]; then
                log_success "Tool usage: $TOOL_USED/$TOOL_TOTAL tools used (${TOOL_ADOPTION}% adoption)"
            else
                log_success "Tool usage analysis completed"
            fi
            echo "  â†³ Detailed report: .claude/tool-analytics.md"
            add_stat "Tools: $TOOL_USED/$TOOL_TOTAL used | ${TOOL_ADOPTION}% adoption"
        fi
    else
        log_warning "Tool usage analysis had issues (non-critical)"
    fi
else
    log_warning "Claude automation had issues (check logs)"
fi
fi  # End dry-run skip for Phase 4-6

# === PHASE 7: Changelog Draft Generation ===
# Generates draft changelog entries from commits since last update
# Exit on failure: No (non-critical, system already activated)
# User interaction: Yes (review and approve draft)
# Skip with: --quick flag
echo ""

if [ "$QUICK" = true ]; then
    log_step "Skipping changelog generation (--quick mode)"
elif dry_run_skip "generate changelog draft from recent commits"; then
    : # skip
else
    log_step "Checking for changelog updates..."
    # Track last processed commit to avoid re-proposing same changes
    CHANGELOG_STATE_FILE=".claude/.changelog-last-processed"
    LAST_PROCESSED_COMMIT=""

    if [ -f "$CHANGELOG_STATE_FILE" ]; then
        LAST_PROCESSED_COMMIT=$(cat "$CHANGELOG_STATE_FILE" 2>/dev/null | head -1)
    fi

    # Count new commits since last processed (or since last dated release as fallback)
    if [ -n "$LAST_PROCESSED_COMMIT" ] && git rev-parse --verify "$LAST_PROCESSED_COMMIT" >/dev/null 2>&1; then
        NEW_COMMITS=$(git rev-list "$LAST_PROCESSED_COMMIT"..HEAD --count 2>/dev/null || echo "0")
        SINCE_MARKER="commit ${LAST_PROCESSED_COMMIT:0:7}"
    else
        # Fallback to dated release if no state file
        LAST_CHANGELOG_DATE=$(grep -oP '^\#\# \[\d{4}-\d{2}-\d{2}\]' CHANGELOG.md 2>/dev/null | head -1 | grep -oP '\d{4}-\d{2}-\d{2}' || echo "")
        if [ -n "$LAST_CHANGELOG_DATE" ]; then
            NEW_COMMITS=$(git log --since="$LAST_CHANGELOG_DATE" --oneline 2>/dev/null | wc -l)
            SINCE_MARKER="$LAST_CHANGELOG_DATE"
        else
            NEW_COMMITS=0
            SINCE_MARKER="unknown"
        fi
    fi

    if [ "$NEW_COMMITS" -gt 0 ]; then
            echo "  Found $NEW_COMMITS commits since $SINCE_MARKER"
            if prompt_user "  Generate changelog draft? [y/N]"; then
                # Run changelog generator script (pass commit hash for accurate filtering)
                if [ -x "./scripts/generate-changelog-draft.sh" ]; then
                    if ./scripts/generate-changelog-draft.sh "$LAST_PROCESSED_COMMIT" > /tmp/changelog-draft-$$.md 2>/dev/null; then
                        echo ""
                        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Changelog Draft â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
                        bat --style=plain --paging=never /tmp/changelog-draft-$$.md 2>/dev/null || cat /tmp/changelog-draft-$$.md
                        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
                        echo ""
                        echo "Options:"
                        echo "  1) Append to CHANGELOG.md [Unreleased]"
                        echo "  2) Edit first in \$EDITOR, then append"
                        echo "  3) Skip (keep draft at /tmp/changelog-draft-$$.md)"
                        read -p "Choose option (1-3): " -n 1 -r CHANGELOG_ACTION
                        echo

                        case $CHANGELOG_ACTION in
                            1)
                                # Insert draft after [Unreleased] line (before ### Planned)
                                # Using awk to insert content at the right place
                                awk -v draft="$(cat /tmp/changelog-draft-$$.md)" '
                                    /^### Planned/ && !inserted {
                                        print draft
                                        print ""
                                        inserted=1
                                    }
                                    {print}
                                ' CHANGELOG.md > /tmp/changelog-updated-$$.md && mv /tmp/changelog-updated-$$.md CHANGELOG.md
                                # Save current commit as last processed to avoid re-proposing
                                git rev-parse HEAD > "$CHANGELOG_STATE_FILE"
                                log_success "Added draft to CHANGELOG.md [Unreleased]"
                                add_action "Generated changelog draft ($NEW_COMMITS commits)"
                                rm -f /tmp/changelog-draft-$$.md
                                ;;
                            2)
                                ${EDITOR:-nano} /tmp/changelog-draft-$$.md
                                awk -v draft="$(cat /tmp/changelog-draft-$$.md)" '
                                    /^### Planned/ && !inserted {
                                        print draft
                                        print ""
                                        inserted=1
                                    }
                                    {print}
                                ' CHANGELOG.md > /tmp/changelog-updated-$$.md && mv /tmp/changelog-updated-$$.md CHANGELOG.md
                                # Save current commit as last processed to avoid re-proposing
                                git rev-parse HEAD > "$CHANGELOG_STATE_FILE"
                                log_success "Added edited draft to CHANGELOG.md"
                                add_action "Generated and edited changelog ($NEW_COMMITS commits)"
                                rm -f /tmp/changelog-draft-$$.md
                                ;;
                            *)
                                log_warning "Changelog update skipped (draft saved at /tmp/changelog-draft-$$.md)"
                                ;;
                        esac
                    else
                        log_warning "Changelog generator script failed"
                    fi
                else
                    log_warning "Changelog generator script not found at ./scripts/generate-changelog-draft.sh"
                fi
            else
                log_warning "Changelog update skipped ($NEW_COMMITS pending commits)"
            fi
        else
            log_success "Changelog up to date (no commits since $SINCE_MARKER)"
        fi
fi

# === PHASE 8: Adaptive Learning Cycle ===
# Runs 7 parallel ML analyzers to detect optimization opportunities:
# - Permission patterns (auto-approve repeated commands)
# - MCP server utilization (move to project-level if low usage)
# - Context optimization (remove unused CLAUDE.md sections)
# - Workflow patterns (bundle repeated command sequences)
# - Instruction effectiveness (improve low-compliance policies)
# - Cross-project transfers (apply patterns from similar projects)
# - Meta-learning (calibrate detection thresholds)
# Then runs CLAUDE.md Suggestion Engine to detect:
# - Repeated instructions ("always do X", "never do Y")
# - Corrections ("no, use X instead")
# - Cross-project patterns (â†’ suggest for global CLAUDE.md)
# - Project-specific patterns (â†’ suggest for project CLAUDE.md)
# Exit on failure: No (learning is optional, system already activated)
# User interaction: Yes (interactive approval of suggestions)
# Skip with: --quick flag

if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode (step not counted)
elif dry_run_skip "run adaptive learning cycle (interactive ML analysis)"; then
    step "Adaptive learning"  # Count step even in dry-run
else
    step "Adaptive learning"
    log_step "Running adaptive learning cycle..."
    AUTOMATION_DIR="$HOME/claude-nixos-automation"
    if [[ -d "$AUTOMATION_DIR" ]]; then
        # STEP 0: Ensure Claude Code hooks are deployed to settings.json
        # This ensures the permission_auto_learner hook runs during Claude sessions
        log_step "  Ensuring Claude Code hooks are deployed..."
        if [[ -f "$AUTOMATION_DIR/claude_automation/deployment/hook_deployer.py" ]]; then
            if python3 "$AUTOMATION_DIR/claude_automation/deployment/hook_deployer.py" --deploy-to-settings 2>/dev/null; then
                log_success "  Claude Code hooks verified/deployed"
            else
                log_warning "  Hook deployment check failed (continuing anyway)"
            fi
        fi

        # STEP 1: Update permission approval data from recent Claude Code sessions
        # This extracts tool usage from ~/.claude/projects/*/*.jsonl into the
        # learning system's permission_approvals.jsonl file.
        # Without this step, adaptive learning would run on stale data!
        log_step "  Refreshing permission data from session logs..."
        if [[ -f "$AUTOMATION_DIR/scripts/parse_session_logs.py" ]]; then
            if python3 "$AUTOMATION_DIR/scripts/parse_session_logs.py" > /dev/null 2>&1; then
                log_success "  Permission data refreshed"
            else
                log_warning "  Failed to refresh permission data (continuing anyway)"
            fi
        fi

        # STEP 2: Run adaptive learning on fresh data
        # Run WITHOUT pipes to preserve stdin/stdout for user interaction
        # No timeout for interactive mode (user needs time to review and respond)
        # Suppress devenv startup noise but preserve Python output for user
        if (cd "$AUTOMATION_DIR" && devenv shell -- python -m claude_automation.cli.run_adaptive_learning --interactive 2> >(grep -v "Building shell" >&2)); then
            LEARNING_EXIT=0
            log_success "Adaptive learning cycle complete"
            add_action "Adaptive learning: suggestions reviewed interactively"
        else
            LEARNING_EXIT=$?
            if [ $LEARNING_EXIT -eq 130 ]; then
                log_warning "Adaptive learning interrupted by user (Ctrl+C)"
            else
                log_warning "Adaptive learning had issues (exit code: $LEARNING_EXIT)"
            fi
        fi

        # STEP 3: CLAUDE.md Suggestion Engine (Two-Stage Pipeline)
        # Stage 1: Regex extracts candidate instruction patterns from session logs
        # Stage 2: Claude API analyzes candidates, filters noise, formats suggestions
        # Suggests edits to global CLAUDE-USER-POLICIES.md or project CLAUDE.md
        #
        # Requires: ANTHROPIC_API_KEY environment variable
        # To enable: export ANTHROPIC_API_KEY=sk-ant-... in your shell profile
        log_step "  Analyzing sessions for CLAUDE.md suggestions..."
        if [[ -d "$AUTOMATION_DIR" ]]; then
            # Use nix run to inherit environment (including ANTHROPIC_API_KEY)
            # --skip-if-no-key ensures graceful skip if key not set
            if nix run "${AUTOMATION_DIR}#suggest-claude-md" -- --apply --skip-if-no-key 2>/dev/null; then
                log_success "  CLAUDE.md suggestions reviewed"
                add_action "CLAUDE.md suggestions: reviewed interactively"
            else
                SUGGEST_EXIT=$?
                if [ $SUGGEST_EXIT -eq 130 ]; then
                    log_warning "  CLAUDE.md suggestions interrupted by user (Ctrl+C)"
                elif [ $SUGGEST_EXIT -ne 0 ]; then
                    log_warning "  CLAUDE.md suggestion engine had issues (continuing anyway)"
                fi
            fi
        fi
    else
        log_warning "Adaptive learning not available (automation directory not found)"
    fi
fi

# === PHASE 9: User Acceptance ===
# User tests the activated configuration
# Offers rollback if issues found
echo ""

if [ "$DRY_RUN" = true ]; then
    log_step "Dry-run complete. No changes were made."
else
    log_step "Configuration activated. Please test the changes now."
    if [ "$YES" = true ]; then
        log_step "Auto-accepting configuration (--yes mode)"
        log_success "Changes accepted"
        add_action "Configuration auto-accepted (--yes mode)"
    elif prompt_user "Are you satisfied with the changes? (y/n)"; then
        log_success "Changes accepted"
        add_action "Configuration tested and accepted"
    else
        echo -e "${YELLOW}âš ï¸  WARNING: This will rollback to the PREVIOUS system generation.${NC}"
        if prompt_user "Confirm rollback? (y/n)"; then
            log_step "Rolling back to previous configuration..."
            sudo nixos-rebuild switch --rollback --flake ".#$NIXOS_HOSTNAME"
            log_success "Rollback complete"
        else
            log_warning "Rollback cancelled â€” system remains on new configuration"
        fi
        exit 0
    fi
fi

# === PHASE 10: Generation Cleanup ===
# Deletes old NixOS generations to free disk space
# Options:
#   - Enter specific generation numbers to delete
#   - Enter 'auto' for automatic cleanup (keeps last 5 of each profile type)
# Auto mode cleans:
#   - System generations (/nix/var/nix/profiles/system)
#   - Root channel profiles (accumulate with each nixpkgs update!)
#   - Default profiles
#   - User/home-manager profiles
# Runs garbage collection after deletion
# Exit on failure: No (cleanup is optional)
# Skip with: --quick flag

if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode
elif [ "$DRY_RUN" = false ]; then
    echo ""
    log_step "Listing system generations..."
    sudo nixos-rebuild list-generations

    read -rp "Enter generations to delete (space-separated), 'a' for auto cleanup, or Enter to skip: " generations_to_delete
    if [ -n "$generations_to_delete" ]; then
        # Accept 'a' or 'auto' for automatic cleanup
        if [ "$generations_to_delete" = "auto" ] || [ "$generations_to_delete" = "a" ]; then
            log_step "Running automatic generation cleanup (keeping last 5)..."

            # Clean system generations (keep last 5)
            sudo nix-env -p /nix/var/nix/profiles/system --delete-generations +5 2>/dev/null || true

            # Clean root channel profiles (these accumulate silently!)
            # Each nixpkgs channel update creates a new generation
            ROOT_CHANNELS="/nix/var/nix/profiles/per-user/root/channels"
            if [ -L "$ROOT_CHANNELS" ]; then
                BEFORE=$(find "${ROOT_CHANNELS%/*}" -maxdepth 1 -name "channels-*-link" 2>/dev/null | wc -l)
                sudo nix-env -p "$ROOT_CHANNELS" --delete-generations +5 2>/dev/null || true
                AFTER=$(find "${ROOT_CHANNELS%/*}" -maxdepth 1 -name "channels-*-link" 2>/dev/null | wc -l)
                CLEANED=$((BEFORE - AFTER))
                [ "$CLEANED" -gt 0 ] && echo "  âœ“ Cleaned $CLEANED root channel generations"
            fi

            # Clean default profile generations
            DEFAULT_PROFILE="/nix/var/nix/profiles/default"
            if [ -L "$DEFAULT_PROFILE" ]; then
                BEFORE=$(find "${DEFAULT_PROFILE%/*}" -maxdepth 1 -name "default-*-link" 2>/dev/null | wc -l)
                sudo nix-env -p "$DEFAULT_PROFILE" --delete-generations +5 2>/dev/null || true
                AFTER=$(find "${DEFAULT_PROFILE%/*}" -maxdepth 1 -name "default-*-link" 2>/dev/null | wc -l)
                CLEANED=$((BEFORE - AFTER))
                [ "$CLEANED" -gt 0 ] && echo "  âœ“ Cleaned $CLEANED default profile generations"
            fi

            # Clean user profile generations (home-manager)
            USER_PROFILE="$HOME/.local/state/nix/profiles/profile"
            if [ -L "$USER_PROFILE" ]; then
                BEFORE=$(find "${USER_PROFILE%/*}" -maxdepth 1 -name "profile-*-link" 2>/dev/null | wc -l)
                nix-env -p "$USER_PROFILE" --delete-generations +5 2>/dev/null || true
                AFTER=$(find "${USER_PROFILE%/*}" -maxdepth 1 -name "profile-*-link" 2>/dev/null | wc -l)
                CLEANED=$((BEFORE - AFTER))
                [ "$CLEANED" -gt 0 ] && echo "  âœ“ Cleaned $CLEANED user profile generations"
            fi

            log_success "Automatic cleanup complete (kept last 5 of each profile)"
        else
            log_step "Deleting generations: $generations_to_delete"
            # shellcheck disable=SC2086  # word splitting intended: space-separated generation numbers
            sudo nix-env -p /nix/var/nix/profiles/system --delete-generations $generations_to_delete
            log_success "Selected generations deleted"
        fi

        log_step "Running garbage collection..."

        # Run GC in background, capture output for stats (suppress verbose deletion lines)
        nix-collect-garbage > /tmp/gc-output-$$.tmp 2>&1 &
        GC_PID=$!
        show_progress "garbage-collection" $GC_PID
        # Capture exit code without triggering set -e (wait propagates the child's exit code)
        GC_EXIT=0
        wait $GC_PID || GC_EXIT=$?

        # Extract stats from GC output (last few lines have the summary)
        if [ -f /tmp/gc-output-$$.tmp ]; then
            # Log full output for debugging
            cat /tmp/gc-output-$$.tmp >> "$LOG_FILE"

            # Parse summary stats (format: "X store paths deleted, Y MiB freed")
            GC_SUMMARY=$(tail -5 /tmp/gc-output-$$.tmp | grep -E "store paths deleted|MiB freed|GiB freed" | tail -1)
            HARDLINK_SAVINGS=$(tail -5 /tmp/gc-output-$$.tmp | grep "hard linking saves" | grep -oP '[\d.]+\s*(MiB|GiB)' | tail -1)

            rm -f /tmp/gc-output-$$.tmp
        fi

        if [ $GC_EXIT -eq 0 ]; then
            if [ -n "$GC_SUMMARY" ]; then
                log_success "Garbage collection: $GC_SUMMARY"
            else
                log_success "Garbage collection complete"
            fi
            [ -n "$HARDLINK_SAVINGS" ] && echo "  â†³ Hard linking saves: $HARDLINK_SAVINGS"
        else
            log_warning "Garbage collection finished with issues (check logs)"
        fi
        add_action "Cleaned up old generations"
    else
        log_warning "No generations selected for deletion"
    fi
fi

# === PHASE 11: Disk Space Analysis ===
# Scans for problematic disk usage patterns:
# - Oversized logs (>100MB = investigate)
# - Large Claude sessions (>5MB = stuck session?)
# - Bloated configs (>1MB = corruption?)
# - Old backups (cleanup candidates)
# Checks learning data health (GREEN/YELLOW/RED risk levels)
# Exit on failure: No (analysis only, no changes made)
# Skip with: --quick flag

if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode
else
    echo ""
    log_step "Analyzing disk space for issues..."
declare -a DISK_WARNINGS=()

# Check for oversized log files (>100MB = problematic)
# Note: fd returns 1 when no matches - protect with || true for pipefail
LARGE_LOGS=$( (fd -t f -s +100M '\.log$' ~ --max-depth 4 2>/dev/null || true) | wc -l)
if [ "$LARGE_LOGS" -gt 0 ]; then
    LARGEST_LOG=$( (fd -t f -s +100M '\.log$' ~ --max-depth 4 2>/dev/null || true) | xargs du -h 2>/dev/null | sort -rh | head -1)
    DISK_WARNINGS+=("$LARGE_LOGS log file(s) >100MB: ${LARGEST_LOG}")
fi

# Check learning data disk health (Phase 1.5)
echo ""
log_step "Checking learning data disk health..."

HEALTH_EXIT_CODE=0
# Run health check via nix (the automation package provides this command)
HEALTH_OUTPUT=$(nix run github:jacopone/claude-nixos-automation#check-data-health 2>&1) || HEALTH_EXIT_CODE=$?

# Parse health stats from output
HEALTH_TOTAL_MB=$(echo "$HEALTH_OUTPUT" | grep "Total:" | grep -oP '\d+(?=MB)' | head -1 || echo "?")
HEALTH_SESSIONS=$(echo "$HEALTH_OUTPUT" | grep "sessions" | grep -oP '\d+(?= sessions)' | head -1 || echo "?")
HEALTH_PERCENTAGE=$(echo "$HEALTH_OUTPUT" | grep "Usage:" | grep -oP '[\d.]+(?=% of disk)' || echo "?")

echo "  â€¢ Total learning data: ${HEALTH_TOTAL_MB}MB"
echo "  â€¢ Claude sessions: ${HEALTH_SESSIONS}"
echo "  â€¢ Disk usage: ${HEALTH_PERCENTAGE}% of total disk"

if [ $HEALTH_EXIT_CODE -eq 2 ]; then
    echo -e "${RED}  ðŸ”´ URGENT: Learning data disk usage critical!${NC}"
    DISK_WARNINGS+=("Learning data usage critical - run adaptive learning to extract insights, then cleanup")
elif [ $HEALTH_EXIT_CODE -eq 1 ]; then
    echo -e "${YELLOW}  ðŸŸ¡ Learning data growing - consider cleanup after insights extracted${NC}"
else
    echo -e "${GREEN}  âœ… Learning data disk health is good${NC}"
fi

# Still check for very large individual session files (may indicate stuck sessions)
LARGE_CLAUDE=$( (fd -t f -s +5M '\.jsonl$' ~/.claude/projects 2>/dev/null || true) | wc -l)
if [ "$LARGE_CLAUDE" -gt 0 ]; then
    DISK_WARNINGS+=("$LARGE_CLAUDE Claude session(s) >5MB (may indicate stuck sessions)")
fi

# Check for oversized config files (>1MB = investigate)
# Note: fd returns 1 when no matches; grep -v returns 1 when all excluded - protect both
LARGE_CONFIGS=$( (fd -t f -s +1M '\.json$' ~/.config ~/.claude --max-depth 3 2>/dev/null || true) | { grep -v "IndexedDB\|chrome" || true; } | wc -l)
if [ "$LARGE_CONFIGS" -gt 0 ]; then
    DISK_WARNINGS+=("$LARGE_CONFIGS config file(s) >1MB (should be <100KB)")
fi

# Check for old backup files (>1MB)
LARGE_BACKUPS=$( (fd -t f -s +1M -e bak -e backup -e old ~ --max-depth 3 2>/dev/null || true) | wc -l)
if [ "$LARGE_BACKUPS" -gt 0 ]; then
    DISK_WARNINGS+=("$LARGE_BACKUPS backup file(s) >1MB (should archive/delete)")
fi

if [ ${#DISK_WARNINGS[@]} -gt 0 ]; then
    echo ""
    echo -e "${YELLOW}âš ï¸  Disk space issues detected (size vs expected):${NC}"
    for warning in "${DISK_WARNINGS[@]}"; do
        echo "   â€¢ $warning"
        log_warning "Disk: $warning"
    done
    echo ""
    echo "ðŸ’¡ Run these commands to investigate:"
    [ "$LARGE_LOGS" -gt 0 ] && echo "   fd -t f -s +100M '\.log$' ~ --max-depth 4 -x du -h | sort -rh"
    [ "$LARGE_CLAUDE" -gt 0 ] && echo "   fd -t f -s +5M '\.jsonl$' ~/.claude/projects -x du -h | sort -rh"
    [ "$LARGE_CONFIGS" -gt 0 ] && echo "   fd -t f -s +1M '\.json$' ~/.config ~/.claude --max-depth 3 | rg -v 'IndexedDB|chrome'"
    echo ""
fi
fi  # End QUICK mode skip for Phase 11 (Disk Analysis)

# === PHASE 12: Cache Cleanup ===
# Offers to clean large cache directories:
# - UV Python cache, Chrome cache, Yarn/npm/pnpm caches
# User confirms before deletion
# Safe to delete (caches regenerate on demand)
# Exit on failure: No (cleanup is optional)
# Skip with: --quick flag

if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode
elif [ "$DRY_RUN" = false ]; then
    echo ""
    log_step "Cache cleanup options..."
    echo "Large cache directories detected:"
    TOTAL_CACHE_SIZE=0
    if [ -d ~/.cache/uv ]; then
    SIZE=$(du -sb ~/.cache/uv 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - UV Python cache: $(du -sh ~/.cache/uv 2>/dev/null | cut -f1)"
fi
if [ -d ~/.cache/google-chrome ]; then
    SIZE=$(du -sb ~/.cache/google-chrome 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - Google Chrome cache: $(du -sh ~/.cache/google-chrome 2>/dev/null | cut -f1)"
fi
if [ -d ~/.cache/yarn ]; then
    SIZE=$(du -sb ~/.cache/yarn 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - Yarn cache: $(du -sh ~/.cache/yarn 2>/dev/null | cut -f1)"
fi
if [ -d ~/.cache/ms-playwright ]; then
    SIZE=$(du -sb ~/.cache/ms-playwright 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - MS Playwright cache: $(du -sh ~/.cache/ms-playwright 2>/dev/null | cut -f1)"
fi
if [ -d ~/.cache/pnpm ]; then
    SIZE=$(du -sb ~/.cache/pnpm 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - PNPM cache: $(du -sh ~/.cache/pnpm 2>/dev/null | cut -f1)"
fi
if [ -d ~/.npm ]; then
    SIZE=$(du -sb ~/.npm 2>/dev/null | cut -f1 || echo "0")
    SIZE=${SIZE:-0}
    TOTAL_CACHE_SIZE=$((TOTAL_CACHE_SIZE + SIZE))
    echo "  - npm cache: $(du -sh ~/.npm 2>/dev/null | cut -f1)"
fi

TOTAL_CACHE_MB=$((TOTAL_CACHE_SIZE / 1024 / 1024))
if [ $TOTAL_CACHE_MB -gt 0 ]; then
    echo "  Total: ${TOTAL_CACHE_MB}MB"
fi

if prompt_user "Clean these cache directories? (y/n)"; then
    log_step "Cleaning cache directories..."
    [ -d ~/.cache/uv ] && rm -rf ~/.cache/uv && echo "  âœ“ UV cache"
    [ -d ~/.cache/google-chrome ] && rm -rf ~/.cache/google-chrome && echo "  âœ“ Chrome cache"
    [ -d ~/.cache/yarn ] && rm -rf ~/.cache/yarn && echo "  âœ“ Yarn cache"
    [ -d ~/.cache/ms-playwright ] && rm -rf ~/.cache/ms-playwright && echo "  âœ“ Playwright cache"
    [ -d ~/.cache/pnpm ] && rm -rf ~/.cache/pnpm && echo "  âœ“ PNPM cache"
    [ -d ~/.npm ] && rm -rf ~/.npm && echo "  âœ“ npm cache"
    [ -d ~/.cache/chromium ] && rm -rf ~/.cache/chromium && echo "  âœ“ Chromium cache"
    log_success "Cache cleanup complete (freed ~${TOTAL_CACHE_MB}MB)"
    add_action "Cleared ${TOTAL_CACHE_MB}MB of cache"
else
    log_warning "Cache cleanup skipped"
fi
fi  # End dry-run skip for Phase 11

# === PHASE 13: Claude Session Lifecycle Cleanup ===
# Deletes sessions marked as IMPLEMENTED (insights already extracted)
# Session lifecycle: RAW â†’ ANALYZED â†’ INSIGHTS_GENERATED â†’ IMPLEMENTED
# Only IMPLEMENTED sessions are safe to delete (value captured)
# Exit on failure: No (cleanup is optional)
# Skip with: --quick flag
if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode
elif [ "$DRY_RUN" = false ] && [ -d ~/.claude/projects ]; then
    echo ""
    log_step "Checking Claude session lifecycle..."

    # Consolidated Python script for lifecycle operations (stats + cleanup)
    # Saves ~300ms by avoiding second Python interpreter startup
    LIFECYCLE_OUTPUT=$(python3 <<'PYTHON_EOF'
import sys
from pathlib import Path

# Add claude-nixos-automation to path
sys.path.insert(0, str(Path.home() / "claude-nixos-automation"))

try:
    from claude_automation.analyzers.session_lifecycle_tracker import SessionLifecycleTracker

    tracker = SessionLifecycleTracker()
    stats = tracker.get_lifecycle_stats()

    # Print stats in parseable format
    print(f"STATS:TOTAL:{stats.total_sessions}")
    print(f"STATS:RAW:{stats.raw_count}")
    print(f"STATS:ANALYZED:{stats.analyzed_count}")
    print(f"STATS:INSIGHTS_GENERATED:{stats.insights_generated_count}")
    print(f"STATS:IMPLEMENTED:{stats.implemented_count}")
    print(f"STATS:SAFE_TO_CLEANUP:{stats.safe_to_cleanup_count}")
    print(f"STATS:VALUABLE:{stats.valuable_data_count}")

    # Pre-load safe sessions list and write to temp file for potential cleanup
    # This avoids a second Python interpreter startup (saves ~300ms)
    safe_sessions = tracker.get_safe_to_cleanup_sessions()
    safe_count = len(safe_sessions)
    print(f"CLEANUP:READY:{safe_count}")

    if safe_count > 0:
        import tempfile
        import json
        temp_file = Path(tempfile.gettempdir()) / "claude_safe_sessions.json"
        with open(temp_file, 'w') as f:
            json.dump([str(p) for p in safe_sessions], f)
        print(f"CLEANUP:TEMPFILE:{temp_file}")

except Exception as e:
    # Fallback if lifecycle tracking not available
    print(f"ERROR:{e}")
    print("STATS:TOTAL:?")
    print("STATS:SAFE_TO_CLEANUP:0")
    print("STATS:VALUABLE:?")
    print("CLEANUP:READY:0")
PYTHON_EOF
)

    # Parse lifecycle stats (now with STATS: prefix)
    # Note: grep returns 1 if no match - protect with || true for pipefail
    TOTAL_SESSIONS=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:TOTAL:" || true; } | cut -d: -f3)
    RAW_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:RAW:" || true; } | cut -d: -f3)
    ANALYZED_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:ANALYZED:" || true; } | cut -d: -f3)
    INSIGHTS_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:INSIGHTS_GENERATED:" || true; } | cut -d: -f3)
    IMPLEMENTED_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:IMPLEMENTED:" || true; } | cut -d: -f3)
    SAFE_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:SAFE_TO_CLEANUP:" || true; } | cut -d: -f3)
    VALUABLE_COUNT=$(echo "$LIFECYCLE_OUTPUT" | { grep "^STATS:VALUABLE:" || true; } | cut -d: -f3)

    # Display lifecycle statistics
    echo "  ðŸ“‹ Session Lifecycle:"
    echo "     â€¢ Total sessions: ${TOTAL_SESSIONS}"
    echo "     â€¢ RAW (not analyzed): ${RAW_COUNT}"
    echo "     â€¢ ANALYZED: ${ANALYZED_COUNT}"
    echo "     â€¢ INSIGHTS_GENERATED: ${INSIGHTS_COUNT}"
    echo "     â€¢ IMPLEMENTED (safe to cleanup): ${IMPLEMENTED_COUNT}"
    echo "     â€¢ Still valuable: ${VALUABLE_COUNT}"
    echo ""

    # Only offer cleanup if there are IMPLEMENTED sessions
    if [ "$SAFE_COUNT" -gt 0 ]; then
        echo "  ðŸ’¡ ${SAFE_COUNT} sessions marked as IMPLEMENTED can be safely deleted"
        echo "     (Insights have been extracted and applied to system config)"
        if prompt_user "  Delete ${SAFE_COUNT} IMPLEMENTED sessions? [y/N]:"; then
            log_step "Cleaning up IMPLEMENTED sessions..."

            # Extract temp file path from lifecycle output
            TEMP_FILE=$(echo "$LIFECYCLE_OUTPUT" | { grep "^CLEANUP:TEMPFILE:" || true; } | cut -d: -f3)

            # Delete sessions using bash + jq (saves ~300ms vs Python interpreter startup)
            if [ -f "$TEMP_FILE" ]; then
                DELETED_COUNT=0
                FREED_BYTES=0

                # Read session paths from JSON array
                while IFS= read -r session_path; do
                    if [ -f "$session_path" ]; then
                        # Get size before deleting
                        SIZE=$(stat -c%s "$session_path" 2>/dev/null || stat -f%z "$session_path" 2>/dev/null || echo "0")
                        SIZE=${SIZE:-0}

                        # Delete session file
                        if rm -f "$session_path" 2>/dev/null; then
                            FREED_BYTES=$((FREED_BYTES + SIZE))
                            DELETED_COUNT=$((DELETED_COUNT + 1))

                            # Delete sidecar metadata file
                            METADATA_PATH="${session_path}.lifecycle.json"
                            rm -f "$METADATA_PATH" 2>/dev/null
                        fi
                    fi
                done < <(jq -r '.[]' "$TEMP_FILE")

                # Calculate freed MB
                FREED_MB=$((FREED_BYTES / 1024 / 1024))

                # Clean up temp file
                rm -f "$TEMP_FILE"

                # Output in same format for parsing
                CLEANUP_RESULT="DELETED:${DELETED_COUNT}
FREED_MB:${FREED_MB}"
            else
                # Fallback if temp file not found
                CLEANUP_RESULT="DELETED:0
FREED_MB:0"
            fi

            DELETED=$(echo "$CLEANUP_RESULT" | grep "^DELETED:" | cut -d: -f2)
            FREED_MB=$(echo "$CLEANUP_RESULT" | grep "^FREED_MB:" | cut -d: -f2)

            log_success "Deleted ${DELETED} IMPLEMENTED sessions (freed ~${FREED_MB}MB)"
            add_action "Cleaned ${FREED_MB}MB of Claude sessions (lifecycle-based)"
        else
            log_warning "Claude session cleanup skipped"
        fi
    elif [ "$VALUABLE_COUNT" -gt 0 ]; then
        echo -e "${YELLOW}  âš ï¸  No sessions marked as IMPLEMENTED yet${NC}"
        echo "     All $ANALYZED_COUNT sessions have been analyzed"
        echo "     Next: Review insights â†’ Implement changes â†’ Mark sessions as IMPLEMENTED"
    else
        echo -e "${GREEN}  âœ… No Claude sessions found${NC}"
    fi

    # Cleanup orphaned learner counter files (older than 1 day)
    # These accumulate when sessions end unexpectedly
    ORPHAN_COUNT=$(find ~/.claude -maxdepth 1 -name "learner_counter_*" -mtime +1 2>/dev/null | wc -l)
    if [ "$ORPHAN_COUNT" -gt 0 ]; then
        find ~/.claude -maxdepth 1 -name "learner_counter_*" -mtime +1 -delete 2>/dev/null
        log_step "Cleaned $ORPHAN_COUNT orphaned session counter files"
    fi
fi

# === PHASE 14: Claude Backup Cleanup ===
# Cleans .backups/ directory (old Claude Code backup files)
# Options: delete all, delete >7 days old, or skip
# Exit on failure: No (cleanup is optional)
# Skip with: --quick flag
if [ "$QUICK" = true ]; then
    : # Skip silently in quick mode
elif [ "$DRY_RUN" = false ] && [ -d .backups ]; then
    backup_count=$( (fd -t f 'backup-' .backups 2>/dev/null || true) | wc -l)
    if [ "$backup_count" -gt 0 ]; then
        echo ""
        log_step "Found $backup_count Claude backup files in .backups/"
        echo "Options:"
        echo "  1) Delete all backup files (keep only latest version in git)"
        echo "  2) Delete backups older than 7 days"
        echo "  3) Skip cleanup"
        read -p "Choose option (1-3): " -n 1 -r
        echo

        if [[ $REPLY == "1" ]]; then
            # Delete all backup files
            FREED_SPACE=$(fd -t f 'backup-' .backups 2>/dev/null -x du -sb | awk '{sum+=$1} END {print int(sum/1024/1024)}' || echo "0")
            fd -t f 'backup-' .backups 2>/dev/null -X rm
            log_success "Deleted $backup_count backups (freed ~${FREED_SPACE}MB)"
            add_action "Cleaned $backup_count backup files"
        elif [[ $REPLY == "2" ]]; then
            # Delete only old backups
            OLD_COUNT=$( (fd -t f 'backup-' .backups --changed-before 7d 2>/dev/null || true) | wc -l)
            FREED_SPACE=$(fd -t f 'backup-' .backups --changed-before 7d 2>/dev/null -x du -sb | awk '{sum+=$1} END {print int(sum/1024/1024)}' || echo "0")
            fd -t f 'backup-' .backups --changed-before 7d 2>/dev/null -X rm
            log_success "Deleted $OLD_COUNT old backups (freed ~${FREED_SPACE}MB)"
            add_action "Cleaned $OLD_COUNT old backup files"
        else
            log_warning "Claude backup cleanup skipped"
        fi
    fi
fi

# Calculate total duration
REBUILD_END_TIME=$(date +%s)
TOTAL_DURATION=$((REBUILD_END_TIME - REBUILD_START_TIME))
DURATION_MINS=$((TOTAL_DURATION / 60))
DURATION_SECS=$((TOTAL_DURATION % 60))

if [ $DURATION_MINS -gt 0 ]; then
    DURATION_STR="${DURATION_MINS}m ${DURATION_SECS}s"
else
    DURATION_STR="${DURATION_SECS}s"
fi

# Print summary
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
if [ "$DRY_RUN" = true ]; then
echo "â•‘                   Dry-Run Complete                         â•‘"
else
echo "â•‘                   Rebuild Complete                         â•‘"
fi
echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
printf "â•‘  Duration: %-47sâ•‘\n" "$DURATION_STR"
if [ "$QUICK" = true ]; then
printf "â•‘  Mode: %-50sâ•‘\n" "Quick (cleanup skipped)"
fi
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

if [ ${#ACTIONS[@]} -gt 0 ]; then
    echo ""
    echo "ðŸ“‹ Actions:"
    for action in "${ACTIONS[@]}"; do
        echo "   âœ“ $action"
    done
fi

if [ ${#STATS[@]} -gt 0 ]; then
    echo ""
    echo "ðŸ“Š Stats:"
    for stat in "${STATS[@]}"; do
        echo "   â€¢ $stat"
    done
fi

if [ "$AUDIT" = true ]; then
    echo ""
    echo "ðŸ“‹ Audit Files:"
    # shellcheck disable=SC2086,SC2012  # glob expansion intended; ls formatting for display
    ls -la "$AUDIT_DIR"/*-$TIMESTAMP* 2>/dev/null | while read -r line; do
        echo "   $line"
    done || echo "   (none generated)"
fi

if [ ${#WARNINGS[@]} -gt 0 ]; then
    echo ""
    echo -e "${YELLOW}âš ï¸  Warnings:${NC}"
    for warning in "${WARNINGS[@]}"; do
        echo "   â€¢ $warning"
    done
fi

if [ "$VERBOSE" = false ]; then
    echo ""
    echo "ðŸ“ Logs: $LOG_FILE"
fi

echo ""
exit 0
